/lamport/shared/jingxi_chen/miniforge3/envs/pytorch/lib/python3.12/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
myrank: 0 local_rank: 0 device_count: 5 world_size: 1
====================================================================================================
        - platform : local
        - local_rank : 0
        - rank : 0
        - device : cuda:0
        - world_size : 1
        - random_seed : 110
        - lr : 0.0002
        - weight_decay : 0.01
        - correct_bias : True
        - adam_epislon : 1e-06
        - no_decay_bias : False
        - adam_beta1 : 0.9
        - adam_beta2 : 0.999
        - scheduler : linear
        - max_step : None
        - max_epoch : 5
        - warmup_step : 500
        - i_steps : 0
        - i_lrs : 0.00025
        - train_data : ./data/e2e/train.jsonl
        - valid_data : ./data/e2e/valid.jsonl
        - train_batch_size : 8
        - valid_batch_size : 4
        - grad_acc : 1
        - clip : 0.0
        - seq_len : 512
        - model_card : gpt2.md
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin
        - fp16 : False
        - log_interval : 100
        - eval_interval : 2000
        - save_interval : 1000
        - work_dir : ./trained_models/GPT2_M/e2e
        - lora_dim : 4
        - lora_alpha : 32
        - obj : clm
        - lora_dropout : 0.1
        - label_smooth : 0.1
        - roll_interval : -1
        - roll_lr : 1e-05
        - roll_step : 100
        - eval_epoch : 1
        - dist : <module 'torch.distributed' from '/lamport/shared/jingxi_chen/miniforge3/envs/pytorch/lib/python3.12/site-packages/torch/distributed/__init__.py'>
====================================================================================================
Experiment dir : ./trained_models/GPT2_M/e2e
loading model pretrained weight.
/lamport/makkapakka/jingxi_chen/GPT2/NLG/src/gpt2_ft.py:326: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  lm_net.load_weight(torch.load(args.init_checkpoint))
set max_step: 26290
start to train the model................ 1
/lamport/makkapakka/jingxi_chen/GPT2/NLG/src/optimizer.py:117: UserWarning: This overload of addcdiv_ is deprecated:
	addcdiv_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcdiv_(Tensor tensor1, Tensor tensor2, *, Number value = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)
  p.data.addcdiv_(-step_size, exp_avg, denom)
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 716.95 | loss  5.17 | avg loss  5.56 | ppl 258.92
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 709.13 | loss  3.21 | avg loss  3.75 | ppl 42.61
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 696.17 | loss  2.97 | avg loss  3.08 | ppl 21.73
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 694.97 | loss  3.11 | avg loss  2.98 | ppl 19.60
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 694.96 | loss  2.84 | avg loss  2.89 | ppl 17.98
| epoch   1 step      600 |    600 batches | lr 0.000199 | ms/batch 696.17 | loss  2.77 | avg loss  2.83 | ppl 16.89
| epoch   1 step      700 |    700 batches | lr 0.000198 | ms/batch 698.47 | loss  2.88 | avg loss  2.79 | ppl 16.29
| epoch   1 step      800 |    800 batches | lr 0.000198 | ms/batch 708.58 | loss  2.47 | avg loss  2.76 | ppl 15.72
| epoch   1 step      900 |    900 batches | lr 0.000197 | ms/batch 714.28 | loss  2.50 | avg loss  2.74 | ppl 15.52
| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 720.52 | loss  3.18 | avg loss  2.76 | ppl 15.87
saving checkpoint ./trained_models/GPT2_M/e2e/model.1000.pt
| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 725.19 | loss  2.83 | avg loss  2.76 | ppl 15.78
| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 731.19 | loss  2.58 | avg loss  2.75 | ppl 15.67
| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 731.66 | loss  2.61 | avg loss  2.72 | ppl 15.15
| epoch   1 step     1400 |   1400 batches | lr 0.000193 | ms/batch 735.89 | loss  2.69 | avg loss  2.72 | ppl 15.15
| epoch   1 step     1500 |   1500 batches | lr 0.000192 | ms/batch 739.05 | loss  2.67 | avg loss  2.73 | ppl 15.27
| epoch   1 step     1600 |   1600 batches | lr 0.000191 | ms/batch 741.81 | loss  2.69 | avg loss  2.68 | ppl 14.59
| epoch   1 step     1700 |   1700 batches | lr 0.000191 | ms/batch 744.12 | loss  2.57 | avg loss  2.70 | ppl 14.81
| epoch   1 step     1800 |   1800 batches | lr 0.00019 | ms/batch 750.43 | loss  2.55 | avg loss  2.69 | ppl 14.68
| epoch   1 step     1900 |   1900 batches | lr 0.000189 | ms/batch 758.09 | loss  2.69 | avg loss  2.69 | ppl 14.66
| epoch   1 step     2000 |   2000 batches | lr 0.000188 | ms/batch 765.37 | loss  2.51 | avg loss  2.67 | ppl 14.45
saving checkpoint ./trained_models/GPT2_M/e2e/model.2000.pt
/lamport/shared/jingxi_chen/miniforge3/envs/pytorch/lib/python3.12/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
eval samples: 0 loss: tensor(1.3171, device='cuda:0')
eval samples: 100 loss: tensor(0.9517, device='cuda:0')
eval samples: 200 loss: tensor(1.3919, device='cuda:0')
eval samples: 300 loss: tensor(1.1741, device='cuda:0')
eval samples: 400 loss: tensor(0.9266, device='cuda:0')
eval samples: 500 loss: tensor(0.7302, device='cuda:0')
eval samples: 600 loss: tensor(1.3091, device='cuda:0')
eval samples: 700 loss: tensor(0.8325, device='cuda:0')
eval samples: 800 loss: tensor(1.2195, device='cuda:0')
eval samples: 900 loss: tensor(1.7497, device='cuda:0')
eval samples: 1000 loss: tensor(1.1771, device='cuda:0')
eval samples: 1100 loss: tensor(1.4759, device='cuda:0')
average loss 1.313362304682601
----------------------------------------------------------------------------------------------------
| Eval   1 at step     2000 | time: 190.12s | valid loss  1.31 | valid ppl  3.72 | best ppl  3.72 
----------------------------------------------------------------------------------------------------
| epoch   1 step     2100 |   2100 batches | lr 0.000188 | ms/batch 2597.10 | loss  2.69 | avg loss  2.67 | ppl 14.47
| epoch   1 step     2200 |   2200 batches | lr 0.000187 | ms/batch 694.00 | loss  2.92 | avg loss  2.66 | ppl 14.29
| epoch   1 step     2300 |   2300 batches | lr 0.000186 | ms/batch 694.07 | loss  2.68 | avg loss  2.65 | ppl 14.18
| epoch   1 step     2400 |   2400 batches | lr 0.000185 | ms/batch 696.93 | loss  2.41 | avg loss  2.66 | ppl 14.24
| epoch   1 step     2500 |   2500 batches | lr 0.000184 | ms/batch 704.92 | loss  2.83 | avg loss  2.67 | ppl 14.49
| epoch   1 step     2600 |   2600 batches | lr 0.000184 | ms/batch 713.26 | loss  2.80 | avg loss  2.65 | ppl 14.22
| epoch   1 step     2700 |   2700 batches | lr 0.000183 | ms/batch 717.24 | loss  2.71 | avg loss  2.65 | ppl 14.10
| epoch   1 step     2800 |   2800 batches | lr 0.000182 | ms/batch 722.31 | loss  2.39 | avg loss  2.68 | ppl 14.55
| epoch   1 step     2900 |   2900 batches | lr 0.000181 | ms/batch 727.96 | loss  2.84 | avg loss  2.66 | ppl 14.36
| epoch   1 step     3000 |   3000 batches | lr 0.000181 | ms/batch 732.50 | loss  2.67 | avg loss  2.63 | ppl 13.83
saving checkpoint ./trained_models/GPT2_M/e2e/model.3000.pt
| epoch   1 step     3100 |   3100 batches | lr 0.00018 | ms/batch 734.97 | loss  2.89 | avg loss  2.63 | ppl 13.85
| epoch   1 step     3200 |   3200 batches | lr 0.000179 | ms/batch 736.16 | loss  2.70 | avg loss  2.66 | ppl 14.32
| epoch   1 step     3300 |   3300 batches | lr 0.000178 | ms/batch 739.50 | loss  3.02 | avg loss  2.62 | ppl 13.72
| epoch   1 step     3400 |   3400 batches | lr 0.000178 | ms/batch 744.27 | loss  2.56 | avg loss  2.62 | ppl 13.73
| epoch   1 step     3500 |   3500 batches | lr 0.000177 | ms/batch 747.71 | loss  2.38 | avg loss  2.65 | ppl 14.12
| epoch   1 step     3600 |   3600 batches | lr 0.000176 | ms/batch 751.50 | loss  2.53 | avg loss  2.65 | ppl 14.21
| epoch   1 step     3700 |   3700 batches | lr 0.000175 | ms/batch 761.16 | loss  2.60 | avg loss  2.62 | ppl 13.79
| epoch   1 step     3800 |   3800 batches | lr 0.000174 | ms/batch 746.94 | loss  2.59 | avg loss  2.58 | ppl 13.17
| epoch   1 step     3900 |   3900 batches | lr 0.000174 | ms/batch 703.94 | loss  2.33 | avg loss  2.66 | ppl 14.25
| epoch   1 step     4000 |   4000 batches | lr 0.000173 | ms/batch 697.58 | loss  2.66 | avg loss  2.65 | ppl 14.18
saving checkpoint ./trained_models/GPT2_M/e2e/model.4000.pt
eval samples: 0 loss: tensor(1.2496, device='cuda:0')
eval samples: 100 loss: tensor(0.9270, device='cuda:0')
eval samples: 200 loss: tensor(1.3620, device='cuda:0')
eval samples: 300 loss: tensor(1.1363, device='cuda:0')
eval samples: 400 loss: tensor(0.9222, device='cuda:0')
eval samples: 500 loss: tensor(0.6494, device='cuda:0')
eval samples: 600 loss: tensor(1.2487, device='cuda:0')
eval samples: 700 loss: tensor(0.7691, device='cuda:0')
eval samples: 800 loss: tensor(1.2012, device='cuda:0')
eval samples: 900 loss: tensor(1.7757, device='cuda:0')
eval samples: 1000 loss: tensor(1.0534, device='cuda:0')
eval samples: 1100 loss: tensor(1.4723, device='cuda:0')
average loss 1.266424267055237
----------------------------------------------------------------------------------------------------
| Eval   2 at step     4000 | time: 182.25s | valid loss  1.27 | valid ppl  3.55 | best ppl  3.55 
----------------------------------------------------------------------------------------------------
| epoch   1 step     4100 |   4100 batches | lr 0.000172 | ms/batch 2520.19 | loss  2.71 | avg loss  2.60 | ppl 13.49
| epoch   1 step     4200 |   4200 batches | lr 0.000171 | ms/batch 702.65 | loss  2.69 | avg loss  2.61 | ppl 13.63
| epoch   1 step     4300 |   4300 batches | lr 0.000171 | ms/batch 711.86 | loss  2.43 | avg loss  2.57 | ppl 13.04
| epoch   1 step     4400 |   4400 batches | lr 0.00017 | ms/batch 716.74 | loss  2.66 | avg loss  2.60 | ppl 13.45
| epoch   1 step     4500 |   4500 batches | lr 0.000169 | ms/batch 721.51 | loss  2.77 | avg loss  2.63 | ppl 13.83
| epoch   1 step     4600 |   4600 batches | lr 0.000168 | ms/batch 726.74 | loss  2.58 | avg loss  2.62 | ppl 13.79
| epoch   1 step     4700 |   4700 batches | lr 0.000167 | ms/batch 731.30 | loss  2.39 | avg loss  2.62 | ppl 13.70
| epoch   1 step     4800 |   4800 batches | lr 0.000167 | ms/batch 734.51 | loss  2.84 | avg loss  2.59 | ppl 13.30
| epoch   1 step     4900 |   4900 batches | lr 0.000166 | ms/batch 736.30 | loss  2.65 | avg loss  2.60 | ppl 13.51
| epoch   1 step     5000 |   5000 batches | lr 0.000165 | ms/batch 738.60 | loss  2.35 | avg loss  2.58 | ppl 13.16
saving checkpoint ./trained_models/GPT2_M/e2e/model.5000.pt
| epoch   1 step     5100 |   5100 batches | lr 0.000164 | ms/batch 742.20 | loss  2.66 | avg loss  2.61 | ppl 13.62
| epoch   1 step     5200 |   5200 batches | lr 0.000164 | ms/batch 745.10 | loss  2.76 | avg loss  2.60 | ppl 13.43
saving checkpoint ./trained_models/GPT2_M/e2e/model.5258.pt
start to train the model................ 2
| epoch   2 step     5300 |     42 batches | lr 0.000163 | ms/batch 305.31 | loss  2.46 | avg loss  2.54 | ppl 12.67
| epoch   2 step     5400 |    142 batches | lr 0.000162 | ms/batch 753.64 | loss  2.50 | avg loss  2.60 | ppl 13.48
| epoch   2 step     5500 |    242 batches | lr 0.000161 | ms/batch 752.90 | loss  2.59 | avg loss  2.59 | ppl 13.27
| epoch   2 step     5600 |    342 batches | lr 0.00016 | ms/batch 710.67 | loss  2.21 | avg loss  2.60 | ppl 13.46
| epoch   2 step     5700 |    442 batches | lr 0.00016 | ms/batch 698.69 | loss  2.54 | avg loss  2.59 | ppl 13.39
| epoch   2 step     5800 |    542 batches | lr 0.000159 | ms/batch 695.35 | loss  2.60 | avg loss  2.57 | ppl 13.02
| epoch   2 step     5900 |    642 batches | lr 0.000158 | ms/batch 701.31 | loss  2.66 | avg loss  2.59 | ppl 13.38
| epoch   2 step     6000 |    742 batches | lr 0.000157 | ms/batch 695.62 | loss  2.31 | avg loss  2.56 | ppl 12.89
saving checkpoint ./trained_models/GPT2_M/e2e/model.6000.pt
eval samples: 0 loss: tensor(1.2258, device='cuda:0')
eval samples: 100 loss: tensor(0.9076, device='cuda:0')
eval samples: 200 loss: tensor(1.2476, device='cuda:0')
eval samples: 300 loss: tensor(1.1602, device='cuda:0')
eval samples: 400 loss: tensor(0.8926, device='cuda:0')
eval samples: 500 loss: tensor(0.6239, device='cuda:0')
eval samples: 600 loss: tensor(1.2303, device='cuda:0')
eval samples: 700 loss: tensor(0.7601, device='cuda:0')
eval samples: 800 loss: tensor(1.1752, device='cuda:0')
eval samples: 900 loss: tensor(1.7924, device='cuda:0')
eval samples: 1000 loss: tensor(1.0321, device='cuda:0')
eval samples: 1100 loss: tensor(1.4715, device='cuda:0')
average loss 1.2334490173483548
----------------------------------------------------------------------------------------------------
| Eval   3 at step     6000 | time: 185.59s | valid loss  1.23 | valid ppl  3.43 | best ppl  3.43 
----------------------------------------------------------------------------------------------------
| epoch   2 step     6100 |    842 batches | lr 0.000157 | ms/batch 2577.32 | loss  2.34 | avg loss  2.58 | ppl 13.17
| epoch   2 step     6200 |    942 batches | lr 0.000156 | ms/batch 721.05 | loss  2.43 | avg loss  2.58 | ppl 13.17
| epoch   2 step     6300 |   1042 batches | lr 0.000155 | ms/batch 731.67 | loss  2.80 | avg loss  2.58 | ppl 13.20
| epoch   2 step     6400 |   1142 batches | lr 0.000154 | ms/batch 731.42 | loss  2.37 | avg loss  2.58 | ppl 13.14
| epoch   2 step     6500 |   1242 batches | lr 0.000153 | ms/batch 739.71 | loss  2.42 | avg loss  2.60 | ppl 13.40
| epoch   2 step     6600 |   1342 batches | lr 0.000153 | ms/batch 737.31 | loss  2.86 | avg loss  2.57 | ppl 13.04
| epoch   2 step     6700 |   1442 batches | lr 0.000152 | ms/batch 742.18 | loss  2.50 | avg loss  2.58 | ppl 13.13
| epoch   2 step     6800 |   1542 batches | lr 0.000151 | ms/batch 743.97 | loss  2.56 | avg loss  2.59 | ppl 13.28
| epoch   2 step     6900 |   1642 batches | lr 0.00015 | ms/batch 748.94 | loss  2.49 | avg loss  2.53 | ppl 12.54
| epoch   2 step     7000 |   1742 batches | lr 0.00015 | ms/batch 749.71 | loss  2.71 | avg loss  2.58 | ppl 13.16
saving checkpoint ./trained_models/GPT2_M/e2e/model.7000.pt
| epoch   2 step     7100 |   1842 batches | lr 0.000149 | ms/batch 759.26 | loss  2.70 | avg loss  2.57 | ppl 13.06
| epoch   2 step     7200 |   1942 batches | lr 0.000148 | ms/batch 756.79 | loss  2.62 | avg loss  2.57 | ppl 13.02
| epoch   2 step     7300 |   2042 batches | lr 0.000147 | ms/batch 711.12 | loss  2.43 | avg loss  2.54 | ppl 12.70
| epoch   2 step     7400 |   2142 batches | lr 0.000146 | ms/batch 704.93 | loss  2.91 | avg loss  2.54 | ppl 12.67
| epoch   2 step     7500 |   2242 batches | lr 0.000146 | ms/batch 695.99 | loss  2.70 | avg loss  2.59 | ppl 13.33
| epoch   2 step     7600 |   2342 batches | lr 0.000145 | ms/batch 700.96 | loss  2.37 | avg loss  2.55 | ppl 12.77
| epoch   2 step     7700 |   2442 batches | lr 0.000144 | ms/batch 695.63 | loss  2.73 | avg loss  2.55 | ppl 12.79
| epoch   2 step     7800 |   2542 batches | lr 0.000143 | ms/batch 704.71 | loss  2.59 | avg loss  2.54 | ppl 12.62
| epoch   2 step     7900 |   2642 batches | lr 0.000143 | ms/batch 708.02 | loss  2.86 | avg loss  2.57 | ppl 13.12
| epoch   2 step     8000 |   2742 batches | lr 0.000142 | ms/batch 717.62 | loss  2.61 | avg loss  2.59 | ppl 13.33
saving checkpoint ./trained_models/GPT2_M/e2e/model.8000.pt
eval samples: 0 loss: tensor(1.1906, device='cuda:0')
eval samples: 100 loss: tensor(0.8998, device='cuda:0')
eval samples: 200 loss: tensor(1.2688, device='cuda:0')
eval samples: 300 loss: tensor(1.1024, device='cuda:0')
eval samples: 400 loss: tensor(0.8943, device='cuda:0')
eval samples: 500 loss: tensor(0.6529, device='cuda:0')
eval samples: 600 loss: tensor(1.1895, device='cuda:0')
eval samples: 700 loss: tensor(0.7483, device='cuda:0')
eval samples: 800 loss: tensor(1.1613, device='cuda:0')
eval samples: 900 loss: tensor(1.7647, device='cuda:0')
eval samples: 1000 loss: tensor(1.0234, device='cuda:0')
eval samples: 1100 loss: tensor(1.4448, device='cuda:0')
average loss 1.2176579923356232
----------------------------------------------------------------------------------------------------
| Eval   4 at step     8000 | time: 190.26s | valid loss  1.22 | valid ppl  3.38 | best ppl  3.38 
----------------------------------------------------------------------------------------------------
| epoch   2 step     8100 |   2842 batches | lr 0.000141 | ms/batch 2627.69 | loss  2.41 | avg loss  2.55 | ppl 12.82
| epoch   2 step     8200 |   2942 batches | lr 0.00014 | ms/batch 732.98 | loss  2.62 | avg loss  2.60 | ppl 13.47
| epoch   2 step     8300 |   3042 batches | lr 0.00014 | ms/batch 735.59 | loss  2.46 | avg loss  2.57 | ppl 13.05
| epoch   2 step     8400 |   3142 batches | lr 0.000139 | ms/batch 736.64 | loss  2.58 | avg loss  2.55 | ppl 12.86
| epoch   2 step     8500 |   3242 batches | lr 0.000138 | ms/batch 740.43 | loss  2.52 | avg loss  2.56 | ppl 12.94
| epoch   2 step     8600 |   3342 batches | lr 0.000137 | ms/batch 744.78 | loss  2.57 | avg loss  2.54 | ppl 12.73
| epoch   2 step     8700 |   3442 batches | lr 0.000136 | ms/batch 747.40 | loss  2.84 | avg loss  2.55 | ppl 12.75
| epoch   2 step     8800 |   3542 batches | lr 0.000136 | ms/batch 760.35 | loss  2.44 | avg loss  2.54 | ppl 12.74
| epoch   2 step     8900 |   3642 batches | lr 0.000135 | ms/batch 736.59 | loss  2.62 | avg loss  2.57 | ppl 13.08
| epoch   2 step     9000 |   3742 batches | lr 0.000134 | ms/batch 706.82 | loss  2.56 | avg loss  2.56 | ppl 12.90
saving checkpoint ./trained_models/GPT2_M/e2e/model.9000.pt
| epoch   2 step     9100 |   3842 batches | lr 0.000133 | ms/batch 700.56 | loss  2.49 | avg loss  2.55 | ppl 12.83
| epoch   2 step     9200 |   3942 batches | lr 0.000133 | ms/batch 699.29 | loss  2.67 | avg loss  2.55 | ppl 12.87
| epoch   2 step     9300 |   4042 batches | lr 0.000132 | ms/batch 697.63 | loss  2.53 | avg loss  2.57 | ppl 13.04
| epoch   2 step     9400 |   4142 batches | lr 0.000131 | ms/batch 695.48 | loss  2.57 | avg loss  2.55 | ppl 12.82
| epoch   2 step     9500 |   4242 batches | lr 0.00013 | ms/batch 706.33 | loss  2.57 | avg loss  2.55 | ppl 12.81
| epoch   2 step     9600 |   4342 batches | lr 0.000129 | ms/batch 707.14 | loss  2.89 | avg loss  2.56 | ppl 12.93
| epoch   2 step     9700 |   4442 batches | lr 0.000129 | ms/batch 720.08 | loss  2.32 | avg loss  2.54 | ppl 12.70
| epoch   2 step     9800 |   4542 batches | lr 0.000128 | ms/batch 718.21 | loss  2.54 | avg loss  2.54 | ppl 12.65
| epoch   2 step     9900 |   4642 batches | lr 0.000127 | ms/batch 728.59 | loss  2.25 | avg loss  2.52 | ppl 12.48
| epoch   2 step    10000 |   4742 batches | lr 0.000126 | ms/batch 727.37 | loss  2.64 | avg loss  2.56 | ppl 12.97
saving checkpoint ./trained_models/GPT2_M/e2e/model.10000.pt
eval samples: 0 loss: tensor(1.1742, device='cuda:0')
eval samples: 100 loss: tensor(0.8736, device='cuda:0')
eval samples: 200 loss: tensor(1.2661, device='cuda:0')
eval samples: 300 loss: tensor(1.0902, device='cuda:0')
eval samples: 400 loss: tensor(0.8781, device='cuda:0')
eval samples: 500 loss: tensor(0.5776, device='cuda:0')
eval samples: 600 loss: tensor(1.1573, device='cuda:0')
eval samples: 700 loss: tensor(0.6941, device='cuda:0')
eval samples: 800 loss: tensor(1.1474, device='cuda:0')
eval samples: 900 loss: tensor(1.7702, device='cuda:0')
eval samples: 1000 loss: tensor(0.9772, device='cuda:0')
eval samples: 1100 loss: tensor(1.4796, device='cuda:0')
average loss 1.1969080164977541
----------------------------------------------------------------------------------------------------
| Eval   5 at step    10000 | time: 192.69s | valid loss  1.20 | valid ppl  3.31 | best ppl  3.31 
----------------------------------------------------------------------------------------------------
| epoch   2 step    10100 |   4842 batches | lr 0.000126 | ms/batch 2667.66 | loss  2.34 | avg loss  2.55 | ppl 12.77
| epoch   2 step    10200 |   4942 batches | lr 0.000125 | ms/batch 738.71 | loss  2.47 | avg loss  2.55 | ppl 12.85
| epoch   2 step    10300 |   5042 batches | lr 0.000124 | ms/batch 750.48 | loss  2.82 | avg loss  2.54 | ppl 12.64
| epoch   2 step    10400 |   5142 batches | lr 0.000123 | ms/batch 745.32 | loss  2.70 | avg loss  2.54 | ppl 12.63
| epoch   2 step    10500 |   5242 batches | lr 0.000122 | ms/batch 754.47 | loss  2.46 | avg loss  2.52 | ppl 12.37
saving checkpoint ./trained_models/GPT2_M/e2e/model.10516.pt
start to train the model................ 3
| epoch   3 step    10600 |     84 batches | lr 0.000122 | ms/batch 620.65 | loss  2.20 | avg loss  2.53 | ppl 12.51
| epoch   3 step    10700 |    184 batches | lr 0.000121 | ms/batch 721.23 | loss  2.60 | avg loss  2.54 | ppl 12.72
| epoch   3 step    10800 |    284 batches | lr 0.00012 | ms/batch 699.06 | loss  2.75 | avg loss  2.53 | ppl 12.51
| epoch   3 step    10900 |    384 batches | lr 0.000119 | ms/batch 703.07 | loss  2.83 | avg loss  2.53 | ppl 12.54
| epoch   3 step    11000 |    484 batches | lr 0.000119 | ms/batch 694.94 | loss  2.66 | avg loss  2.53 | ppl 12.50
saving checkpoint ./trained_models/GPT2_M/e2e/model.11000.pt
| epoch   3 step    11100 |    584 batches | lr 0.000118 | ms/batch 701.99 | loss  2.59 | avg loss  2.58 | ppl 13.20
| epoch   3 step    11200 |    684 batches | lr 0.000117 | ms/batch 697.80 | loss  2.58 | avg loss  2.53 | ppl 12.59
| epoch   3 step    11300 |    784 batches | lr 0.000116 | ms/batch 706.74 | loss  2.58 | avg loss  2.55 | ppl 12.81
| epoch   3 step    11400 |    884 batches | lr 0.000115 | ms/batch 713.70 | loss  2.54 | avg loss  2.54 | ppl 12.67
| epoch   3 step    11500 |    984 batches | lr 0.000115 | ms/batch 719.36 | loss  2.27 | avg loss  2.54 | ppl 12.72
| epoch   3 step    11600 |   1084 batches | lr 0.000114 | ms/batch 722.82 | loss  2.19 | avg loss  2.50 | ppl 12.15
| epoch   3 step    11700 |   1184 batches | lr 0.000113 | ms/batch 724.26 | loss  2.36 | avg loss  2.52 | ppl 12.43
| epoch   3 step    11800 |   1284 batches | lr 0.000112 | ms/batch 735.39 | loss  2.54 | avg loss  2.50 | ppl 12.14
| epoch   3 step    11900 |   1384 batches | lr 0.000112 | ms/batch 732.80 | loss  2.42 | avg loss  2.51 | ppl 12.30
| epoch   3 step    12000 |   1484 batches | lr 0.000111 | ms/batch 741.54 | loss  2.57 | avg loss  2.52 | ppl 12.48
saving checkpoint ./trained_models/GPT2_M/e2e/model.12000.pt
eval samples: 0 loss: tensor(1.1947, device='cuda:0')
eval samples: 100 loss: tensor(0.8810, device='cuda:0')
eval samples: 200 loss: tensor(1.2737, device='cuda:0')
eval samples: 300 loss: tensor(1.1137, device='cuda:0')
eval samples: 400 loss: tensor(0.9212, device='cuda:0')
eval samples: 500 loss: tensor(0.5932, device='cuda:0')
eval samples: 600 loss: tensor(1.1462, device='cuda:0')
eval samples: 700 loss: tensor(0.7151, device='cuda:0')
eval samples: 800 loss: tensor(1.1622, device='cuda:0')
eval samples: 900 loss: tensor(1.7883, device='cuda:0')
eval samples: 1000 loss: tensor(0.9460, device='cuda:0')
eval samples: 1100 loss: tensor(1.4863, device='cuda:0')
average loss 1.2096580245009023
----------------------------------------------------------------------------------------------------
| Eval   6 at step    12000 | time: 194.87s | valid loss  1.21 | valid ppl  3.35 | best ppl  3.35 
----------------------------------------------------------------------------------------------------
| epoch   3 step    12100 |   1584 batches | lr 0.00011 | ms/batch 2696.93 | loss  2.50 | avg loss  2.54 | ppl 12.65
| epoch   3 step    12200 |   1684 batches | lr 0.000109 | ms/batch 746.85 | loss  2.46 | avg loss  2.55 | ppl 12.80
| epoch   3 step    12300 |   1784 batches | lr 0.000108 | ms/batch 754.74 | loss  2.49 | avg loss  2.53 | ppl 12.61
| epoch   3 step    12400 |   1884 batches | lr 0.000108 | ms/batch 735.41 | loss  2.57 | avg loss  2.50 | ppl 12.23
| epoch   3 step    12500 |   1984 batches | lr 0.000107 | ms/batch 706.44 | loss  2.75 | avg loss  2.53 | ppl 12.53
| epoch   3 step    12600 |   2084 batches | lr 0.000106 | ms/batch 700.05 | loss  2.82 | avg loss  2.47 | ppl 11.82
| epoch   3 step    12700 |   2184 batches | lr 0.000105 | ms/batch 695.34 | loss  2.35 | avg loss  2.52 | ppl 12.44
| epoch   3 step    12800 |   2284 batches | lr 0.000105 | ms/batch 701.93 | loss  2.61 | avg loss  2.53 | ppl 12.53
| epoch   3 step    12900 |   2384 batches | lr 0.000104 | ms/batch 695.53 | loss  2.43 | avg loss  2.53 | ppl 12.50
| epoch   3 step    13000 |   2484 batches | lr 0.000103 | ms/batch 705.25 | loss  3.05 | avg loss  2.54 | ppl 12.74
saving checkpoint ./trained_models/GPT2_M/e2e/model.13000.pt
| epoch   3 step    13100 |   2584 batches | lr 0.000102 | ms/batch 705.64 | loss  2.35 | avg loss  2.49 | ppl 12.12
| epoch   3 step    13200 |   2684 batches | lr 0.000102 | ms/batch 718.39 | loss  2.43 | avg loss  2.55 | ppl 12.85
| epoch   3 step    13300 |   2784 batches | lr 0.000101 | ms/batch 716.99 | loss  2.44 | avg loss  2.50 | ppl 12.19
| epoch   3 step    13400 |   2884 batches | lr 0.0001 | ms/batch 727.33 | loss  2.62 | avg loss  2.50 | ppl 12.17
| epoch   3 step    13500 |   2984 batches | lr 9.92e-05 | ms/batch 725.57 | loss  2.89 | avg loss  2.54 | ppl 12.65
| epoch   3 step    13600 |   3084 batches | lr 9.84e-05 | ms/batch 733.69 | loss  2.32 | avg loss  2.53 | ppl 12.58
| epoch   3 step    13700 |   3184 batches | lr 9.76e-05 | ms/batch 734.93 | loss  2.67 | avg loss  2.56 | ppl 12.97
| epoch   3 step    13800 |   3284 batches | lr 9.69e-05 | ms/batch 738.20 | loss  2.29 | avg loss  2.49 | ppl 12.11
| epoch   3 step    13900 |   3384 batches | lr 9.61e-05 | ms/batch 738.42 | loss  2.90 | avg loss  2.54 | ppl 12.70
| epoch   3 step    14000 |   3484 batches | lr 9.53e-05 | ms/batch 743.28 | loss  2.67 | avg loss  2.55 | ppl 12.75
saving checkpoint ./trained_models/GPT2_M/e2e/model.14000.pt
eval samples: 0 loss: tensor(1.1280, device='cuda:0')
eval samples: 100 loss: tensor(0.8506, device='cuda:0')
eval samples: 200 loss: tensor(1.2888, device='cuda:0')
eval samples: 300 loss: tensor(1.0816, device='cuda:0')
eval samples: 400 loss: tensor(0.9044, device='cuda:0')
eval samples: 500 loss: tensor(0.5853, device='cuda:0')
eval samples: 600 loss: tensor(1.1764, device='cuda:0')
eval samples: 700 loss: tensor(0.7288, device='cuda:0')
eval samples: 800 loss: tensor(1.1296, device='cuda:0')
eval samples: 900 loss: tensor(1.7747, device='cuda:0')
eval samples: 1000 loss: tensor(0.9532, device='cuda:0')
eval samples: 1100 loss: tensor(1.4758, device='cuda:0')
average loss 1.1881369738393042
----------------------------------------------------------------------------------------------------
| Eval   7 at step    14000 | time: 196.78s | valid loss  1.19 | valid ppl  3.28 | best ppl  3.28 
----------------------------------------------------------------------------------------------------
| epoch   3 step    14100 |   3584 batches | lr 9.45e-05 | ms/batch 2716.70 | loss  2.57 | avg loss  2.48 | ppl 11.89
| epoch   3 step    14200 |   3684 batches | lr 9.38e-05 | ms/batch 733.36 | loss  2.52 | avg loss  2.50 | ppl 12.13
| epoch   3 step    14300 |   3784 batches | lr 9.3e-05 | ms/batch 701.84 | loss  2.75 | avg loss  2.50 | ppl 12.19
| epoch   3 step    14400 |   3884 batches | lr 9.22e-05 | ms/batch 703.01 | loss  2.26 | avg loss  2.51 | ppl 12.30
| epoch   3 step    14500 |   3984 batches | lr 9.14e-05 | ms/batch 695.52 | loss  2.38 | avg loss  2.53 | ppl 12.51
| epoch   3 step    14600 |   4084 batches | lr 9.07e-05 | ms/batch 699.04 | loss  2.51 | avg loss  2.53 | ppl 12.60
| epoch   3 step    14700 |   4184 batches | lr 8.99e-05 | ms/batch 698.79 | loss  2.49 | avg loss  2.50 | ppl 12.14
| epoch   3 step    14800 |   4284 batches | lr 8.91e-05 | ms/batch 702.89 | loss  2.51 | avg loss  2.54 | ppl 12.62
| epoch   3 step    14900 |   4384 batches | lr 8.83e-05 | ms/batch 712.34 | loss  2.51 | avg loss  2.51 | ppl 12.28
| epoch   3 step    15000 |   4484 batches | lr 8.76e-05 | ms/batch 713.41 | loss  2.79 | avg loss  2.54 | ppl 12.72
saving checkpoint ./trained_models/GPT2_M/e2e/model.15000.pt
| epoch   3 step    15100 |   4584 batches | lr 8.68e-05 | ms/batch 725.21 | loss  2.34 | avg loss  2.51 | ppl 12.26
| epoch   3 step    15200 |   4684 batches | lr 8.6e-05 | ms/batch 721.93 | loss  2.52 | avg loss  2.50 | ppl 12.18
| epoch   3 step    15300 |   4784 batches | lr 8.52e-05 | ms/batch 733.00 | loss  2.46 | avg loss  2.54 | ppl 12.69
| epoch   3 step    15400 |   4884 batches | lr 8.45e-05 | ms/batch 730.10 | loss  2.38 | avg loss  2.52 | ppl 12.41
| epoch   3 step    15500 |   4984 batches | lr 8.37e-05 | ms/batch 739.01 | loss  2.42 | avg loss  2.52 | ppl 12.38
| epoch   3 step    15600 |   5084 batches | lr 8.29e-05 | ms/batch 735.47 | loss  2.64 | avg loss  2.50 | ppl 12.23
| epoch   3 step    15700 |   5184 batches | lr 8.21e-05 | ms/batch 742.78 | loss  2.47 | avg loss  2.48 | ppl 11.96
saving checkpoint ./trained_models/GPT2_M/e2e/model.15774.pt
start to train the model................ 4
| epoch   4 step    15800 |     26 batches | lr 8.13e-05 | ms/batch 185.25 | loss  2.36 | avg loss  2.49 | ppl 12.09
| epoch   4 step    15900 |    126 batches | lr 8.06e-05 | ms/batch 749.02 | loss  2.37 | avg loss  2.49 | ppl 12.11
| epoch   4 step    16000 |    226 batches | lr 7.98e-05 | ms/batch 743.73 | loss  2.51 | avg loss  2.52 | ppl 12.39
saving checkpoint ./trained_models/GPT2_M/e2e/model.16000.pt
eval samples: 0 loss: tensor(1.1476, device='cuda:0')
eval samples: 100 loss: tensor(0.8718, device='cuda:0')
eval samples: 200 loss: tensor(1.2399, device='cuda:0')
eval samples: 300 loss: tensor(1.0932, device='cuda:0')
eval samples: 400 loss: tensor(0.8880, device='cuda:0')
eval samples: 500 loss: tensor(0.5548, device='cuda:0')
eval samples: 600 loss: tensor(1.1334, device='cuda:0')
eval samples: 700 loss: tensor(0.6791, device='cuda:0')
eval samples: 800 loss: tensor(1.1603, device='cuda:0')
eval samples: 900 loss: tensor(1.7635, device='cuda:0')
eval samples: 1000 loss: tensor(0.9196, device='cuda:0')
eval samples: 1100 loss: tensor(1.4431, device='cuda:0')
average loss 1.1829000125090554
----------------------------------------------------------------------------------------------------
| Eval   8 at step    16000 | time: 192.83s | valid loss  1.18 | valid ppl  3.26 | best ppl  3.26 
----------------------------------------------------------------------------------------------------
| epoch   4 step    16100 |    326 batches | lr 7.9e-05 | ms/batch 2634.52 | loss  2.73 | avg loss  2.49 | ppl 12.02
| epoch   4 step    16200 |    426 batches | lr 7.82e-05 | ms/batch 696.28 | loss  2.44 | avg loss  2.50 | ppl 12.13
| epoch   4 step    16300 |    526 batches | lr 7.75e-05 | ms/batch 702.65 | loss  2.86 | avg loss  2.48 | ppl 11.99
| epoch   4 step    16400 |    626 batches | lr 7.67e-05 | ms/batch 695.73 | loss  2.91 | avg loss  2.50 | ppl 12.13
| epoch   4 step    16500 |    726 batches | lr 7.59e-05 | ms/batch 703.46 | loss  2.72 | avg loss  2.53 | ppl 12.50
| epoch   4 step    16600 |    826 batches | lr 7.51e-05 | ms/batch 700.04 | loss  2.72 | avg loss  2.49 | ppl 12.10
| epoch   4 step    16700 |    926 batches | lr 7.44e-05 | ms/batch 704.63 | loss  2.66 | avg loss  2.51 | ppl 12.28
| epoch   4 step    16800 |   1026 batches | lr 7.36e-05 | ms/batch 709.31 | loss  2.69 | avg loss  2.51 | ppl 12.26
| epoch   4 step    16900 |   1126 batches | lr 7.28e-05 | ms/batch 716.19 | loss  2.19 | avg loss  2.48 | ppl 11.89
| epoch   4 step    17000 |   1226 batches | lr 7.2e-05 | ms/batch 718.90 | loss  2.34 | avg loss  2.47 | ppl 11.85
saving checkpoint ./trained_models/GPT2_M/e2e/model.17000.pt
| epoch   4 step    17100 |   1326 batches | lr 7.13e-05 | ms/batch 724.04 | loss  2.61 | avg loss  2.53 | ppl 12.54
| epoch   4 step    17200 |   1426 batches | lr 7.05e-05 | ms/batch 726.95 | loss  2.40 | avg loss  2.47 | ppl 11.83
| epoch   4 step    17300 |   1526 batches | lr 6.97e-05 | ms/batch 727.37 | loss  2.36 | avg loss  2.50 | ppl 12.24
| epoch   4 step    17400 |   1626 batches | lr 6.89e-05 | ms/batch 738.96 | loss  2.30 | avg loss  2.50 | ppl 12.22
| epoch   4 step    17500 |   1726 batches | lr 6.82e-05 | ms/batch 730.86 | loss  2.69 | avg loss  2.51 | ppl 12.36
| epoch   4 step    17600 |   1826 batches | lr 6.74e-05 | ms/batch 742.30 | loss  2.82 | avg loss  2.52 | ppl 12.40
| epoch   4 step    17700 |   1926 batches | lr 6.66e-05 | ms/batch 736.97 | loss  2.39 | avg loss  2.49 | ppl 12.04
| epoch   4 step    17800 |   2026 batches | lr 6.58e-05 | ms/batch 744.86 | loss  2.73 | avg loss  2.53 | ppl 12.52
| epoch   4 step    17900 |   2126 batches | lr 6.51e-05 | ms/batch 741.30 | loss  2.50 | avg loss  2.49 | ppl 12.01
| epoch   4 step    18000 |   2226 batches | lr 6.43e-05 | ms/batch 751.41 | loss  3.00 | avg loss  2.50 | ppl 12.14
saving checkpoint ./trained_models/GPT2_M/e2e/model.18000.pt
eval samples: 0 loss: tensor(1.1340, device='cuda:0')
eval samples: 100 loss: tensor(0.8774, device='cuda:0')
eval samples: 200 loss: tensor(1.2546, device='cuda:0')
eval samples: 300 loss: tensor(1.0900, device='cuda:0')
eval samples: 400 loss: tensor(0.8931, device='cuda:0')
eval samples: 500 loss: tensor(0.5827, device='cuda:0')
eval samples: 600 loss: tensor(1.1545, device='cuda:0')
eval samples: 700 loss: tensor(0.6969, device='cuda:0')
eval samples: 800 loss: tensor(1.1154, device='cuda:0')
eval samples: 900 loss: tensor(1.7505, device='cuda:0')
eval samples: 1000 loss: tensor(0.9309, device='cuda:0')
eval samples: 1100 loss: tensor(1.4736, device='cuda:0')
average loss 1.1835783874641543
----------------------------------------------------------------------------------------------------
| Eval   9 at step    18000 | time: 193.06s | valid loss  1.18 | valid ppl  3.27 | best ppl  3.26 
----------------------------------------------------------------------------------------------------
| epoch   4 step    18100 |   2326 batches | lr 6.35e-05 | ms/batch 2632.85 | loss  2.62 | avg loss  2.49 | ppl 12.08
| epoch   4 step    18200 |   2426 batches | lr 6.27e-05 | ms/batch 698.16 | loss  2.39 | avg loss  2.50 | ppl 12.17
| epoch   4 step    18300 |   2526 batches | lr 6.2e-05 | ms/batch 694.44 | loss  2.38 | avg loss  2.51 | ppl 12.26
| epoch   4 step    18400 |   2626 batches | lr 6.12e-05 | ms/batch 701.93 | loss  2.58 | avg loss  2.48 | ppl 11.98
| epoch   4 step    18500 |   2726 batches | lr 6.04e-05 | ms/batch 697.25 | loss  2.49 | avg loss  2.47 | ppl 11.84
| epoch   4 step    18600 |   2826 batches | lr 5.96e-05 | ms/batch 710.00 | loss  2.31 | avg loss  2.52 | ppl 12.42
| epoch   4 step    18700 |   2926 batches | lr 5.89e-05 | ms/batch 710.99 | loss  2.27 | avg loss  2.48 | ppl 11.97
| epoch   4 step    18800 |   3026 batches | lr 5.81e-05 | ms/batch 720.84 | loss  2.28 | avg loss  2.51 | ppl 12.33
| epoch   4 step    18900 |   3126 batches | lr 5.73e-05 | ms/batch 719.23 | loss  2.47 | avg loss  2.49 | ppl 12.10
| epoch   4 step    19000 |   3226 batches | lr 5.65e-05 | ms/batch 727.02 | loss  2.76 | avg loss  2.50 | ppl 12.20
saving checkpoint ./trained_models/GPT2_M/e2e/model.19000.pt
| epoch   4 step    19100 |   3326 batches | lr 5.58e-05 | ms/batch 728.89 | loss  2.75 | avg loss  2.48 | ppl 12.00
| epoch   4 step    19200 |   3426 batches | lr 5.5e-05 | ms/batch 734.96 | loss  2.58 | avg loss  2.48 | ppl 11.90
| epoch   4 step    19300 |   3526 batches | lr 5.42e-05 | ms/batch 734.73 | loss  2.55 | avg loss  2.51 | ppl 12.29
| epoch   4 step    19400 |   3626 batches | lr 5.34e-05 | ms/batch 738.93 | loss  2.65 | avg loss  2.49 | ppl 12.03
| epoch   4 step    19500 |   3726 batches | lr 5.27e-05 | ms/batch 737.97 | loss  2.23 | avg loss  2.49 | ppl 12.04
| epoch   4 step    19600 |   3826 batches | lr 5.19e-05 | ms/batch 743.08 | loss  2.65 | avg loss  2.50 | ppl 12.14
| epoch   4 step    19700 |   3926 batches | lr 5.11e-05 | ms/batch 745.04 | loss  2.61 | avg loss  2.49 | ppl 12.03
| epoch   4 step    19800 |   4026 batches | lr 5.03e-05 | ms/batch 743.03 | loss  2.48 | avg loss  2.50 | ppl 12.15
| epoch   4 step    19900 |   4126 batches | lr 4.96e-05 | ms/batch 753.09 | loss  2.33 | avg loss  2.53 | ppl 12.58
| epoch   4 step    20000 |   4226 batches | lr 4.88e-05 | ms/batch 724.96 | loss  2.39 | avg loss  2.50 | ppl 12.13
saving checkpoint ./trained_models/GPT2_M/e2e/model.20000.pt
eval samples: 0 loss: tensor(1.1651, device='cuda:0')
eval samples: 100 loss: tensor(0.9009, device='cuda:0')
eval samples: 200 loss: tensor(1.2471, device='cuda:0')
eval samples: 300 loss: tensor(1.0784, device='cuda:0')
eval samples: 400 loss: tensor(0.8859, device='cuda:0')
eval samples: 500 loss: tensor(0.5728, device='cuda:0')
eval samples: 600 loss: tensor(1.1389, device='cuda:0')
eval samples: 700 loss: tensor(0.7023, device='cuda:0')
eval samples: 800 loss: tensor(1.1239, device='cuda:0')
eval samples: 900 loss: tensor(1.7471, device='cuda:0')
eval samples: 1000 loss: tensor(0.9332, device='cuda:0')
eval samples: 1100 loss: tensor(1.4664, device='cuda:0')
average loss 1.178931405385063
----------------------------------------------------------------------------------------------------
| Eval  10 at step    20000 | time: 184.03s | valid loss  1.18 | valid ppl  3.25 | best ppl  3.25 
----------------------------------------------------------------------------------------------------
| epoch   4 step    20100 |   4326 batches | lr 4.8e-05 | ms/batch 2538.15 | loss  2.38 | avg loss  2.49 | ppl 12.02
| epoch   4 step    20200 |   4426 batches | lr 4.72e-05 | ms/batch 699.57 | loss  2.51 | avg loss  2.50 | ppl 12.14
| epoch   4 step    20300 |   4526 batches | lr 4.65e-05 | ms/batch 700.60 | loss  2.48 | avg loss  2.51 | ppl 12.36
| epoch   4 step    20400 |   4626 batches | lr 4.57e-05 | ms/batch 700.83 | loss  2.45 | avg loss  2.49 | ppl 12.09
| epoch   4 step    20500 |   4726 batches | lr 4.49e-05 | ms/batch 712.59 | loss  2.67 | avg loss  2.47 | ppl 11.85
| epoch   4 step    20600 |   4826 batches | lr 4.41e-05 | ms/batch 711.72 | loss  2.64 | avg loss  2.52 | ppl 12.40
| epoch   4 step    20700 |   4926 batches | lr 4.34e-05 | ms/batch 722.56 | loss  2.61 | avg loss  2.48 | ppl 11.98
| epoch   4 step    20800 |   5026 batches | lr 4.26e-05 | ms/batch 720.05 | loss  2.29 | avg loss  2.51 | ppl 12.33
| epoch   4 step    20900 |   5126 batches | lr 4.18e-05 | ms/batch 729.89 | loss  2.42 | avg loss  2.48 | ppl 11.94
| epoch   4 step    21000 |   5226 batches | lr 4.1e-05 | ms/batch 727.64 | loss  2.30 | avg loss  2.48 | ppl 11.96
saving checkpoint ./trained_models/GPT2_M/e2e/model.21000.pt
saving checkpoint ./trained_models/GPT2_M/e2e/model.21032.pt
start to train the model................ 5
| epoch   5 step    21100 |     68 batches | lr 4.02e-05 | ms/batch 495.12 | loss  2.64 | avg loss  2.51 | ppl 12.32
| epoch   5 step    21200 |    168 batches | lr 3.95e-05 | ms/batch 731.01 | loss  2.60 | avg loss  2.45 | ppl 11.57
| epoch   5 step    21300 |    268 batches | lr 3.87e-05 | ms/batch 741.47 | loss  2.28 | avg loss  2.50 | ppl 12.21
| epoch   5 step    21400 |    368 batches | lr 3.79e-05 | ms/batch 737.93 | loss  2.54 | avg loss  2.49 | ppl 12.05
| epoch   5 step    21500 |    468 batches | lr 3.71e-05 | ms/batch 743.95 | loss  2.70 | avg loss  2.50 | ppl 12.16
| epoch   5 step    21600 |    568 batches | lr 3.64e-05 | ms/batch 742.97 | loss  2.78 | avg loss  2.46 | ppl 11.68
| epoch   5 step    21700 |    668 batches | lr 3.56e-05 | ms/batch 745.56 | loss  2.81 | avg loss  2.49 | ppl 12.12
| epoch   5 step    21800 |    768 batches | lr 3.48e-05 | ms/batch 716.41 | loss  2.41 | avg loss  2.51 | ppl 12.35
| epoch   5 step    21900 |    868 batches | lr 3.4e-05 | ms/batch 703.19 | loss  2.72 | avg loss  2.49 | ppl 12.03
| epoch   5 step    22000 |    968 batches | lr 3.33e-05 | ms/batch 699.00 | loss  2.38 | avg loss  2.44 | ppl 11.43
saving checkpoint ./trained_models/GPT2_M/e2e/model.22000.pt
eval samples: 0 loss: tensor(1.1297, device='cuda:0')
eval samples: 100 loss: tensor(0.8726, device='cuda:0')
eval samples: 200 loss: tensor(1.2433, device='cuda:0')
eval samples: 300 loss: tensor(1.0768, device='cuda:0')
eval samples: 400 loss: tensor(0.8596, device='cuda:0')
eval samples: 500 loss: tensor(0.5725, device='cuda:0')
eval samples: 600 loss: tensor(1.1384, device='cuda:0')
eval samples: 700 loss: tensor(0.6887, device='cuda:0')
eval samples: 800 loss: tensor(1.1522, device='cuda:0')
eval samples: 900 loss: tensor(1.7401, device='cuda:0')
eval samples: 1000 loss: tensor(0.8957, device='cuda:0')
eval samples: 1100 loss: tensor(1.4731, device='cuda:0')
average loss 1.1716861172694049
----------------------------------------------------------------------------------------------------
| Eval  11 at step    22000 | time: 182.83s | valid loss  1.17 | valid ppl  3.23 | best ppl  3.23 
----------------------------------------------------------------------------------------------------
| epoch   5 step    22100 |   1068 batches | lr 3.25e-05 | ms/batch 2529.74 | loss  2.59 | avg loss  2.47 | ppl 11.78
| epoch   5 step    22200 |   1168 batches | lr 3.17e-05 | ms/batch 702.31 | loss  2.89 | avg loss  2.49 | ppl 12.09
| epoch   5 step    22300 |   1268 batches | lr 3.09e-05 | ms/batch 706.47 | loss  2.29 | avg loss  2.50 | ppl 12.14
| epoch   5 step    22400 |   1368 batches | lr 3.02e-05 | ms/batch 711.59 | loss  2.32 | avg loss  2.44 | ppl 11.53
| epoch   5 step    22500 |   1468 batches | lr 2.94e-05 | ms/batch 713.11 | loss  2.53 | avg loss  2.48 | ppl 12.00
| epoch   5 step    22600 |   1568 batches | lr 2.86e-05 | ms/batch 723.61 | loss  2.69 | avg loss  2.50 | ppl 12.16
| epoch   5 step    22700 |   1668 batches | lr 2.78e-05 | ms/batch 720.82 | loss  2.38 | avg loss  2.47 | ppl 11.87
| epoch   5 step    22800 |   1768 batches | lr 2.71e-05 | ms/batch 731.07 | loss  2.32 | avg loss  2.49 | ppl 12.05
| epoch   5 step    22900 |   1868 batches | lr 2.63e-05 | ms/batch 727.46 | loss  2.88 | avg loss  2.53 | ppl 12.53
| epoch   5 step    23000 |   1968 batches | lr 2.55e-05 | ms/batch 738.06 | loss  2.35 | avg loss  2.48 | ppl 11.89
saving checkpoint ./trained_models/GPT2_M/e2e/model.23000.pt
| epoch   5 step    23100 |   2068 batches | lr 2.47e-05 | ms/batch 730.56 | loss  2.55 | avg loss  2.51 | ppl 12.25
| epoch   5 step    23200 |   2168 batches | lr 2.4e-05 | ms/batch 741.83 | loss  2.66 | avg loss  2.47 | ppl 11.87
| epoch   5 step    23300 |   2268 batches | lr 2.32e-05 | ms/batch 736.69 | loss  2.35 | avg loss  2.45 | ppl 11.62
| epoch   5 step    23400 |   2368 batches | lr 2.24e-05 | ms/batch 744.55 | loss  2.37 | avg loss  2.48 | ppl 11.97
| epoch   5 step    23500 |   2468 batches | lr 2.16e-05 | ms/batch 741.82 | loss  2.33 | avg loss  2.46 | ppl 11.70
| epoch   5 step    23600 |   2568 batches | lr 2.09e-05 | ms/batch 748.14 | loss  2.21 | avg loss  2.47 | ppl 11.85
| epoch   5 step    23700 |   2668 batches | lr 2.01e-05 | ms/batch 745.63 | loss  2.31 | avg loss  2.45 | ppl 11.59
| epoch   5 step    23800 |   2768 batches | lr 1.93e-05 | ms/batch 728.93 | loss  2.59 | avg loss  2.49 | ppl 12.09
| epoch   5 step    23900 |   2868 batches | lr 1.85e-05 | ms/batch 700.90 | loss  2.54 | avg loss  2.46 | ppl 11.75
| epoch   5 step    24000 |   2968 batches | lr 1.78e-05 | ms/batch 700.88 | loss  2.34 | avg loss  2.48 | ppl 12.00
saving checkpoint ./trained_models/GPT2_M/e2e/model.24000.pt
eval samples: 0 loss: tensor(1.1324, device='cuda:0')
eval samples: 100 loss: tensor(0.8749, device='cuda:0')
eval samples: 200 loss: tensor(1.2375, device='cuda:0')
eval samples: 300 loss: tensor(1.0856, device='cuda:0')
eval samples: 400 loss: tensor(0.8712, device='cuda:0')
eval samples: 500 loss: tensor(0.5653, device='cuda:0')
eval samples: 600 loss: tensor(1.1551, device='cuda:0')
eval samples: 700 loss: tensor(0.6949, device='cuda:0')
eval samples: 800 loss: tensor(1.1386, device='cuda:0')
eval samples: 900 loss: tensor(1.7524, device='cuda:0')
eval samples: 1000 loss: tensor(0.9063, device='cuda:0')
eval samples: 1100 loss: tensor(1.4843, device='cuda:0')
average loss 1.171140715224694
----------------------------------------------------------------------------------------------------
| Eval  12 at step    24000 | time: 182.97s | valid loss  1.17 | valid ppl  3.23 | best ppl  3.23 
----------------------------------------------------------------------------------------------------
| epoch   5 step    24100 |   3068 batches | lr 1.7e-05 | ms/batch 2529.17 | loss  2.66 | avg loss  2.51 | ppl 12.29
| epoch   5 step    24200 |   3168 batches | lr 1.62e-05 | ms/batch 711.18 | loss  2.63 | avg loss  2.44 | ppl 11.47
| epoch   5 step    24300 |   3268 batches | lr 1.54e-05 | ms/batch 711.97 | loss  2.62 | avg loss  2.49 | ppl 12.05
| epoch   5 step    24400 |   3368 batches | lr 1.47e-05 | ms/batch 719.40 | loss  2.62 | avg loss  2.48 | ppl 11.89
| epoch   5 step    24500 |   3468 batches | lr 1.39e-05 | ms/batch 722.05 | loss  2.37 | avg loss  2.47 | ppl 11.82
| epoch   5 step    24600 |   3568 batches | lr 1.31e-05 | ms/batch 726.75 | loss  2.58 | avg loss  2.47 | ppl 11.86
| epoch   5 step    24700 |   3668 batches | lr 1.23e-05 | ms/batch 728.68 | loss  2.83 | avg loss  2.51 | ppl 12.36
| epoch   5 step    24800 |   3768 batches | lr 1.16e-05 | ms/batch 732.51 | loss  2.84 | avg loss  2.47 | ppl 11.82
| epoch   5 step    24900 |   3868 batches | lr 1.08e-05 | ms/batch 737.31 | loss  2.30 | avg loss  2.48 | ppl 11.89
| epoch   5 step    25000 |   3968 batches | lr 1e-05 | ms/batch 732.41 | loss  2.35 | avg loss  2.48 | ppl 11.89
saving checkpoint ./trained_models/GPT2_M/e2e/model.25000.pt
| epoch   5 step    25100 |   4068 batches | lr 9.23e-06 | ms/batch 743.55 | loss  2.59 | avg loss  2.46 | ppl 11.74
| epoch   5 step    25200 |   4168 batches | lr 8.45e-06 | ms/batch 738.96 | loss  2.35 | avg loss  2.49 | ppl 12.03
| epoch   5 step    25300 |   4268 batches | lr 7.68e-06 | ms/batch 748.18 | loss  2.42 | avg loss  2.46 | ppl 11.68
| epoch   5 step    25400 |   4368 batches | lr 6.9e-06 | ms/batch 743.71 | loss  2.70 | avg loss  2.45 | ppl 11.55
| epoch   5 step    25500 |   4468 batches | lr 6.13e-06 | ms/batch 745.66 | loss  2.49 | avg loss  2.48 | ppl 11.90
| epoch   5 step    25600 |   4568 batches | lr 5.35e-06 | ms/batch 708.64 | loss  2.70 | avg loss  2.48 | ppl 11.94
| epoch   5 step    25700 |   4668 batches | lr 4.58e-06 | ms/batch 704.41 | loss  2.76 | avg loss  2.52 | ppl 12.46
| epoch   5 step    25800 |   4768 batches | lr 3.8e-06 | ms/batch 696.25 | loss  2.52 | avg loss  2.50 | ppl 12.18
| epoch   5 step    25900 |   4868 batches | lr 3.02e-06 | ms/batch 701.37 | loss  2.34 | avg loss  2.47 | ppl 11.81
| epoch   5 step    26000 |   4968 batches | lr 2.25e-06 | ms/batch 696.04 | loss  2.40 | avg loss  2.49 | ppl 12.06
saving checkpoint ./trained_models/GPT2_M/e2e/model.26000.pt
eval samples: 0 loss: tensor(1.1394, device='cuda:0')
eval samples: 100 loss: tensor(0.8770, device='cuda:0')
eval samples: 200 loss: tensor(1.2364, device='cuda:0')
eval samples: 300 loss: tensor(1.0788, device='cuda:0')
eval samples: 400 loss: tensor(0.8795, device='cuda:0')
eval samples: 500 loss: tensor(0.5580, device='cuda:0')
eval samples: 600 loss: tensor(1.1320, device='cuda:0')
eval samples: 700 loss: tensor(0.6872, device='cuda:0')
eval samples: 800 loss: tensor(1.1251, device='cuda:0')
eval samples: 900 loss: tensor(1.7403, device='cuda:0')
eval samples: 1000 loss: tensor(0.9061, device='cuda:0')
eval samples: 1100 loss: tensor(1.4675, device='cuda:0')
average loss 1.1666677914406747
----------------------------------------------------------------------------------------------------
| Eval  13 at step    26000 | time: 185.10s | valid loss  1.17 | valid ppl  3.21 | best ppl  3.21 
----------------------------------------------------------------------------------------------------
| epoch   5 step    26100 |   5068 batches | lr 1.47e-06 | ms/batch 2569.51 | loss  2.48 | avg loss  2.47 | ppl 11.84
| epoch   5 step    26200 |   5168 batches | lr 6.98e-07 | ms/batch 716.74 | loss  2.59 | avg loss  2.46 | ppl 11.69
saving checkpoint ./trained_models/GPT2_M/e2e/model.26290.pt
----------------------------------------------------------------------------------------------------
End of training
